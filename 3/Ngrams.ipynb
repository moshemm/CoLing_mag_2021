{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковое моделирование заключается в приписывании вероятности последовательности слов. Сейчас языковые модели используются практически во всех nlp задачах. Всякие Берты и Элмо - языковые модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это достаточно сложная тема, поэтому будем разбирать постепенно. Сегодня разберём самые основы. Научимся приписывать вероятность последовательности слов и попробуем генерировать текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем два разных корпуса: новостной и сообщения с vk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk = open('vk_texts_norm.txt').read()\n",
    "news = open('lenta2.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По длине оно сопоставимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина Вк ---- 5497266\n",
      "Длина Лента - 5897301\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина Вк ----\", len(vk))\n",
    "print(\"Длина Лента -\", len(news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую функцию для нормализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним тексты по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vk = normalize(vk)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса ВКонтакте текстов в токенах -  848722\n",
      "Длина корпуса новостных текстов в токенах -  766869\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса ВКонтакте текстов в токенах - \", len(norm_vk))\n",
    "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И по уникальным токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных токенов в корпусе ВКонтакте -  114023\n",
      "Уникальный токенов в корпусе Ленты -  79054\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных токенов в корпусе ВКонтакте - \", len(set(norm_vk)))\n",
    "print(\"Уникальный токенов в корпусе Ленты - \", len(set(norm_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, сколько раз встречаются слова и выведем самые частотные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_vk = Counter(norm_vk)\n",
    "vocab_news = Counter(norm_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 32672),\n",
       " ('в', 21946),\n",
       " ('не', 13763),\n",
       " ('на', 10688),\n",
       " ('с', 8435),\n",
       " ('что', 7473),\n",
       " ('я', 7424),\n",
       " ('а', 5129),\n",
       " ('это', 4768),\n",
       " ('как', 4621)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_vk.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 35997),\n",
       " ('и', 17120),\n",
       " ('на', 14306),\n",
       " ('по', 10053),\n",
       " ('что', 9136),\n",
       " ('с', 8146),\n",
       " ('не', 6612),\n",
       " ('как', 4126),\n",
       " ('о', 4047),\n",
       " ('из', 3892)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнивать употребимость конкретных слов в разных текстах в абсолютных числах неудобно. Нормализуем счётчики на размеры текстов. Так у нас получается вероятность слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.038495526214708704),\n",
       " ('в', 0.02585770134390295),\n",
       " ('не', 0.01621614615857725),\n",
       " ('на', 0.012593051670629487),\n",
       " ('с', 0.009938472197020933),\n",
       " ('что', 0.008805003287295486),\n",
       " ('я', 0.008747269423910303),\n",
       " ('а', 0.006043203781685876),\n",
       " ('это', 0.005617858380011358),\n",
       " ('как', 0.005444656789855807),\n",
       " ('для', 0.004745959218684092),\n",
       " ('по', 0.004610461376045395),\n",
       " ('https', 0.0041120649635569715),\n",
       " ('к', 0.00406964824760051),\n",
       " ('за', 0.0038917336890053516),\n",
       " ('ты', 0.0038434257624993815),\n",
       " ('от', 0.0038363563098399714),\n",
       " ('но', 0.003751522877927048),\n",
       " ('»', 0.003657263509134911),\n",
       " ('«', 0.003625450972167565)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вк\n",
    "probas_vk = Counter({word:c/len(norm_vk) for word, c in vocab_vk.items()})\n",
    "probas_vk.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 0.04694022055918286),\n",
       " ('и', 0.022324543044509558),\n",
       " ('на', 0.01865507668193655),\n",
       " ('по', 0.013109149020236834),\n",
       " ('что', 0.011913377643378464),\n",
       " ('с', 0.010622413997697129),\n",
       " ('не', 0.008622072348732314),\n",
       " ('как', 0.005380319194021404),\n",
       " ('о', 0.005277302903103399),\n",
       " ('из', 0.005075182332314907),\n",
       " ('к', 0.004138907688275312),\n",
       " ('за', 0.0038911469885990957),\n",
       " ('россии', 0.003790738705046103),\n",
       " ('для', 0.003450393743911933),\n",
       " ('его', 0.003189593007410653),\n",
       " ('от', 0.0031817689853156144),\n",
       " ('он', 0.003166120941125538),\n",
       " ('сообщает', 0.003026592547097353),\n",
       " ('а', 0.002902712197259245),\n",
       " ('также', 0.0027149356669783236)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лента\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем понять, какое предложение относится к Вк, а какое к новостям, если судить по униграммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'\n",
    "\n",
    "prob = Counter({'news':0, 'vk':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['news'] += probas_news.get(word, 0)\n",
    "    prob['vk'] += probas_vk.get(word, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vk', 0.021858747622896545), ('news', 0.014115839863131772)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Четыре сотрудника саратовского УФСИН уволены после сообщений о пытках'\n",
    "\n",
    "prob = Counter({'news':0, 'vk':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['news'] += probas_news.get(word, 0)\n",
    "    prob['vk'] += probas_vk.get(word, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('news', 0.007285468574163254), ('vk', 0.003784513657004296)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты получаются не очень точные. Возможно это из-за того, что мы считаем слова независимыми друг от друга. А это очевидно не так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По-хорошему вероятность последовательности нужно расчитывать по формуле полной вероятности. Но у нас не очень большие тексты и мы не можем получить вероятности для длинных фраз (их просто может не быть в текстах). Поэтому мы воспользуемся предположением Маркова и будем учитывать только предыдущее слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы расчитать вероятность с таким предположением, нам достаточно найти количество вхождений для каждого биграмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    return ngrams(tokens, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы у нас получились честные вероятности и можно было посчитать вероятность первого слова, нам нужно добавить тэг маркирующий начало предложений \\< start \\>\n",
    "\n",
    "Дальше мы попробуем сгенерировать текст, используя эти вероятности, и нам нужно будет когда-то остановится. Для этого добавим тэг окончания \\< end \\>\n",
    "\n",
    "Ну и поделим все на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences_news = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]\n",
    "sentences_vk = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(vk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "\n",
    "    \n",
    "unigrams_vk = Counter()\n",
    "bigrams_vk = Counter()\n",
    "\n",
    "for sentence in sentences_vk:\n",
    "    unigrams_vk.update(sentence)\n",
    "    bigrams_vk.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<start>', 'в'), 3831),\n",
       " (('<start>', 'по'), 3043),\n",
       " (('<start>', 'как'), 2064),\n",
       " (('риа', 'новости'), 1669),\n",
       " (('по', 'словам'), 966),\n",
       " (('что', 'в'), 888),\n",
       " (('об', 'этом'), 871),\n",
       " (('<start>', 'однако'), 845),\n",
       " (('как', 'сообщает'), 809),\n",
       " (('<start>', 'на'), 805)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('https', 'vk'), 2647),\n",
       " (('vk', 'com'), 2389),\n",
       " (('<start>', 'и'), 2209),\n",
       " (('<start>', 'я'), 1630),\n",
       " (('<start>', 'в'), 1611),\n",
       " (('<start>', 'а'), 1272),\n",
       " (('и', 'не'), 1164),\n",
       " (('╬═╬', '╬═╬'), 1154),\n",
       " (('<start>', 'но'), 887),\n",
       " (('<start>', 'не'), 845)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_vk.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать условную вероятность мы можем поделить количество вхождений на количество вхождений первого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'\n",
    "# phrase = 'Ныть надо меньше и работать больше.'\n",
    "# phrase = 'напиши мне в лс тогда и отвечу'\n",
    "phrase = 'Четыре сотрудника саратовского УФСИН уволены после сообщений о пытках'\n",
    "prob = Counter()\n",
    "for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>']):\n",
    "    word1, word2 = ngram[0], ngram[1]\n",
    "    \n",
    "    if word1 in unigrams_vk and ngram in bigrams_vk:\n",
    "        prob['vk'] += np.log(bigrams_vk[ngram]/unigrams_vk[word1])\n",
    "    else:\n",
    "        prob['vk'] += np.log(0.001)\n",
    "    \n",
    "    if word1 in unigrams_news and ngram in bigrams_news:\n",
    "        prob['news'] += np.log(bigrams_news[ngram]/unigrams_news[word1])\n",
    "    else:\n",
    "        prob['news'] += np.log(0.001)\n",
    "\n",
    "prob['news'] = np.exp(prob['news'])\n",
    "prob['vk'] = np.exp(prob['vk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('news', 1.0764269200870128e-26), ('vk', 5.121638924455876e-32)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает получше. Мы воспользовались небольшим хаком - для слов или биграммов, которых не было у нас в словаре, прибавляли низкую вероятность. Надо было делать сглаживания по правилам, но это долго и сложно..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблем с неизвестными словами у нас не будет, если мы будем пытаться сгенерировать новый текст. Давайте попробуем это сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_vk = np.zeros((len(unigrams_vk), \n",
    "                   len(unigrams_vk)))\n",
    "id2word_vk = list(unigrams_vk)\n",
    "word2id_vk = {word:i for i, word in enumerate(id2word_vk)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_vk:\n",
    "    word1, word2 = ngram[0], ngram[1]\n",
    "    matrix_vk[word2id_vk[word1]][word2id_vk[word2]] =  (bigrams_vk[ngram]/\n",
    "                                                                     unigrams_vk[word1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим матрицу вероятностей перейти из одного слова в другое\n",
    "matrix_news = np.zeros((len(unigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "# вероятность расчитываем точно также\n",
    "for ngram in bigrams_news:\n",
    "    word1, word2 = ngram[0], ngram[1]\n",
    "    matrix_news[word2id_news[word1]][word2id_news[word2]] =  (bigrams_news[ngram]/\n",
    "                                                                     unigrams_news[word1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "святой \n",
      " ❗ 💎 открыть для бизнеса классные летние прогулочные коляски авторские методики разработки ижевских оружейников из нее такой подарок в вашем грузовом отсеке побывал в lol \n",
      " – тех \n",
      " думайте письменно пообещал что где-то или пуститься в игре целуй и сверх y многому другому \n",
      " она самый неадекват \n",
      " планировалось назначить женщину обманули предпочли а всё это в ее потерял веру в клубе легко если где-нибудь глубоко в своей маме и kollontai \n",
      " любите друг друга прямо сейчас думаю и делении дробей ⠀ 🆘скажите 👉🏻вам когда деньги не смеяться и понимает \n",
      " начни действовать автоматически только если вы\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_vk, id2word_vk, word2id_vk).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в доме № 783 \n",
      " лидеры стран которые запрещают хранение на заседании совета министров финансов фбр и оформления документов \n",
      " автомобиль lincoln navigator и врезался мини-трактор обслуживавший заправку лайнера погибли 9 кандидатов от эксимбанка сша семьей не пришло подчеркнул что бин ладеном и авторитета михаила леонтьева и все ведущие разработку еще на металлолом \n",
      " во вторник предъявлено так и распространении коммерческих запусков космических аппаратов для партии консерваторов находящаяся в служебной проверки все результаты переговоров с показом 30 рублей право опротестовать вчерашнее решение таганрогской окружной избирательной комиссии проект модернизации почти парализована \n",
      " позже агентство итар-тасс высокопоставленный сотрудник госбезопасности и на каждую\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать триграммную модель на основе кода выше.\n",
    "\n",
    "Подсказки:\n",
    "    - нужно будет добавить еще один тэг <start>\n",
    "    - еще одна матрица не нужна, можно по строкам хранить биграмы, а по колонкам униграммы\n",
    "    - тексты должны быть очень похоже на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
