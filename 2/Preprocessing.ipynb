{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 1. Предобработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале любой работы с текстом, как правило, требуется выполнить одни и теже действия: удалить все лишнее, разбить на предложения, токенизировать, нормализовать. На этом занятии мы разберем каждый из этих этапов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сразу импортируем все нужные библиотеки\n",
    "import string\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re, os, json\n",
    "mystem = Mystem(entire_input=False)\n",
    "morph = MorphAnalyzer()\n",
    "# если есть ошибки, доустановите библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление лишнего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто в данных, с которыми нам нужно работать помимо текста присутствует ещё какая-то лишняя информация: тэги, ссылки, код, разметка. Она, конечно, не всегда лишняя, но обычно от неё лучше избавиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем в качестве примера статью с Хабра (она была в презе). Она скачана автоматически и в там остались некоторые тэги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "<div xmlns=\"http://www.w3.org/1999/xhtml\">Привет! Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP). Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком. Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение. Делимся опытом в этом посте.<br/>\n",
    "<br/><img data-src=\"https://habrastorage.org/getpro/habr/post_images/c87/ec8/f26/c87ec8f26a969cf54915271e24abcba1.png\" src=\"/img/image-loader.svg\"/><br/>\n",
    "<a name=\"habracut\"></a><br/><h2>Подготовка к отбору.</h2><br/>Начнём с основ: как все работает?<br/>Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов. Этапы дальнейшего анализа укладываются в пирамиду:<br/>\n",
    "<br/><img data-src=\"https://habrastorage.org/getpro/habr/post_images/b2f/cd9/0aa/b2fcd90aaf42d1eee5ed3ee84fcf27fd.png\" src=\"/img/image-loader.svg\"/><br/><br/>Начинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.). На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами. Синтаксические парсеры, о которых пойдет речь, анализируют текст и выдают структуру зависимостей слов друг от друга.<br/>\n",
    "<br/><h3>Грамматика зависимостей и грамматика непосредственных составляющих.</h3><br/>\n",
    "Есть два основных подхода к синтаксическому анализу, которые в лингвистической теории существуют примерно на равных.<br/>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В html все тэги заключаются в угловые скобки. Мы можем испьзовать это, чтобы легко избавиться от всех тэгов сразу. Напишем регулярное выражение, которое будет захватывать всё, что попадает между символами < и >, и не является '>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re - модуль регулярных выражений в питоне\n",
    "# функция sub заменяет все, что подходит под шаблон, на указанный текст\n",
    "def remove_tags_1(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает наша функция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Привет! Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP). Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком. Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение. Делимся опытом в этом посте.\n",
      "\n",
      "Подготовка к отбору.Начнём с основ: как все работает?Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов. Этапы дальнейшего анализа укладываются в пирамиду:\n",
      "Начинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.). На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами. Синтаксические парсеры, о которых пойдет речь, анализируют текст и выдают структуру зависимостей слов друг от друга.\n",
      "Грамматика зависимостей и грамматика непосредственных составляющих.\n",
      "Есть два основных подхода к синтаксическому анализу, которые в лингвистической теории существуют примерно на равных.\n"
     ]
    }
   ],
   "source": [
    "print(remove_tags_1(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что в некоторых местах удаление тэгов приводит к тому, что между точкой и началом следующего \n",
    "предложения нет пробела, а это может помешать правильно токенизировать текст,\n",
    "поэтому сделаем так, чтобы тэг заменялся пробелом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags_2(text):\n",
    "    return re.sub(r'<[^>]+>', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Привет! Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP). Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком. Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение. Делимся опытом в этом посте. \n",
      "   \n",
      "    Подготовка к отбору.  Начнём с основ: как все работает? Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов. Этапы дальнейшего анализа укладываются в пирамиду: \n",
      "    Начинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.). На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами. Синтаксические парсеры, о которых пойдет речь, анализируют текст и выдают структуру зависимостей слов друг от друга. \n",
      "  Грамматика зависимостей и грамматика непосредственных составляющих.  \n",
      "Есть два основных подхода к синтаксическому анализу, которые в лингвистической теории существуют примерно на равных. \n"
     ]
    }
   ],
   "source": [
    "print(remove_tags_2(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь куски текста не слипаются, но появились последовательности из нескольких пробелов, чтобы убрать их добавим ещё одно регулярное выражение и применим его к тексту, из которого уже удалили тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags_3(text):\n",
    "    no_tags_text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    no_space_sequences_text = re.sub('  +', ' ', no_tags_text)\n",
    "    return no_space_sequences_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Привет! Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP). Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком. Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение. Делимся опытом в этом посте. \n",
      " \n",
      " Подготовка к отбору. Начнём с основ: как все работает? Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов. Этапы дальнейшего анализа укладываются в пирамиду: \n",
      " Начинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.). На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами. Синтаксические парсеры, о которых пойдет речь, анализируют текст и выдают структуру зависимостей слов друг от друга. \n",
      " Грамматика зависимостей и грамматика непосредственных составляющих. \n",
      "Есть два основных подхода к синтаксическому анализу, которые в лингвистической теории существуют примерно на равных. \n"
     ]
    }
   ],
   "source": [
    "print(remove_tags_3(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь текст более менее чистый. Присвоим его в переменную text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_tags_3(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка текста достаточно индивидуальна и каждый раз приходится писать какие-то новые правила. А вот разбиение на предложения, токенизация и лемматизация - на 90% универсальны (только если вы не работаете с соцсетями!:)), т.е. одни и теже решения будут работать во всех задачах. К тому же, есть уже хорошие готовые инструменты для всего этого. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация (разбиение на предложения)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __nltk__ есть уже готовая функция для разбивки на предложения. (Тут у нас очень просто пример)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n Привет!',\n",
       " 'Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP).',\n",
       " 'Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком.',\n",
       " 'Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение.',\n",
       " 'Делимся опытом в этом посте.',\n",
       " 'Подготовка к отбору.',\n",
       " 'Начнём с основ: как все работает?',\n",
       " 'Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов.',\n",
       " 'Этапы дальнейшего анализа укладываются в пирамиду: \\n Начинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.).',\n",
       " 'На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text, 'russian')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но вы можете придумать и потестить всякие примерчики, которые осложнят жизнь сегментатору. К вопросу о том, как важно понимать, с какими данными вы работаете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['илок.', 'сила']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_text = \"илок. сила\"\n",
    "sent_tokenize(small_text, 'russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запомните: программисты (nlp-инженеры, называйте их как хотите) не любят смотреть в данные. Их больше интересуют их модельки. Смотреть в данные - ВАША задача. Хорошо знайте ваши данные и будет вам КЛ-счастье"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У DeepPavlov есть библиотека [**rusenttokenizer**](https://github.com/deepmipt/ru_sentence_tokenizer). (У меня всегда вылезают варнинги, и не только у меня)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Привет!',\n",
       " 'Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP).',\n",
       " 'Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком.',\n",
       " 'Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение.',\n",
       " 'Делимся опытом в этом посте.',\n",
       " 'Подготовка к отбору.',\n",
       " 'Начнём с основ: как все работает?',\n",
       " 'Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов.',\n",
       " 'Этапы дальнейшего анализа укладываются в пирамиду: \\n Начинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.).',\n",
       " 'На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sent_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Куда? - говорю, - Куда мы едем, батя?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sent_tokenize('- Куда? - говорю, - Куда мы едем, батя?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте Natasha есть библиотека [**razdel**](https://github.com/natasha/razdel). Она чуть более навороченная. Установите Наташу у себя и погоняйте сами, у меня была измененная версия, которую допиливала я."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(sentenize('БЕБИ.РУ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 7, 'БЕБИ.РУ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# числа тут - это спаны, индексы начала и конца предложения в изначальном тексте\n",
    "sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['БЕБИ.РУ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у объекта Substring есть атрибуты start, stop и text. С помощью них можно вытащить нужное\n",
    "[sent.text for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все-таки нужно добавить каких-то специфичных правил разбиения на предложения, можно опять же восползоваться регулярными выражениями. Однако в этом случае регулярка будет посложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы разбили текст на предложения. Теперь предложения нужно разбить на токены. Под токенами обычно понимаются слова (псевдослова), но это могут быть и какие-то более длинные или короткие куски. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ токенизации -- стандартный питоновский __str.split__ метод.\n",
    "    По умолчанию он разбивает текст по последовательностям пробелом "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет!',\n",
       " 'Меня',\n",
       " 'зовут',\n",
       " 'Денис',\n",
       " 'Кирьянов,',\n",
       " 'я',\n",
       " 'работаю',\n",
       " 'в',\n",
       " 'Сбербанке',\n",
       " 'и',\n",
       " 'занимаюсь',\n",
       " 'проблемами',\n",
       " 'обработки',\n",
       " 'естественного',\n",
       " 'языка',\n",
       " '(NLP).',\n",
       " 'Однажды',\n",
       " 'нам',\n",
       " 'понадобилось',\n",
       " 'выбрать']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть слов отделяется, но знаки препинания лепятся к словам.\n",
    "Можно пройтись по всем словам и убрать из них пунктцацию с методом str.strip. Помините, что в каких-то задачах нам нужны знаки препинания, в каких-то нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#основные знаки преминания хранятся в питоновском модуле string.punctuation\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в этом списке не хватает кавычек-ёлочек, лапок, длинного тире и многоточия (из самых стандартных вещей)\n",
    "string.punctuation += '«»—…“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет',\n",
       " 'Меня',\n",
       " 'зовут',\n",
       " 'Денис',\n",
       " 'Кирьянов',\n",
       " 'я',\n",
       " 'работаю',\n",
       " 'в',\n",
       " 'Сбербанке',\n",
       " 'и']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.strip(string.punctuation) for word in text.split()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так не будут удаляться дефисы и точке в сокращениях, не разделенных пробелом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как-нибудь'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'как-нибудь'.strip(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'т.е'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'т.е.'.strip(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё слова можно извлечь с помощью простого регулярного выражения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет',\n",
       " 'Меня',\n",
       " 'зовут',\n",
       " 'Денис',\n",
       " 'Кирьянов',\n",
       " 'я',\n",
       " 'работаю',\n",
       " 'в',\n",
       " 'Сбербанке',\n",
       " 'и']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё есть готовые токенизаторы из nltk. Они не удаляют пунктуацию, а выделяют её отдельным токеном.\n",
    "\n",
    "Например **wordpunct_tokenizer** разбирает по регулярке - *'\\w+|[^\\w\\s]+'* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет', '!', 'Меня', 'зовут', 'Денис', 'Кирьянов', ',', 'я', 'работаю', 'в']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё есть **word_tokenize**. Он также построен на регулярках, но они там более сложные (учитывается последовательность некоторых \n",
    "символов, символы начала, конца слова и т.д). \n",
    "\n",
    "Специально подобранного под русский язык токенизатора там нет, \n",
    "но и с английским всё работает достаточно хорошо --\n",
    "сокращения типа т.к собираются в один токен, дефисные слова тоже не разделяются, многоточия тут тоже не отделяются, но это можно поправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет', '!', 'Меня', 'зовут', 'Денис', 'Кирьянов', ',', 'я', 'работаю', 'в']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vk.ru']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('vk.ru')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в razdel тоже есть токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет', '!', 'Меня', 'зовут', 'Денис', 'Кирьянов', ',', 'я', 'работаю', 'в']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и всегда, думайте, что важно для вашей задачи и смотрите в данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные токены тоже чаще всего нужно привести к какому-то стандартному виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое простое и очевидное - привести всё к одному регистру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'слово'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Слово'.lower()\n",
    "# если не нужно разбивать на предложения, то это можно сделать в самом начале\n",
    "# text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для языков со слабым словоизменением этого может быть достаточно. Но для флективного русского лучше использовать стемминг или лемматизацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг - это урезание слова до его \"основы\" (стема), т.е. такой части, которая является общей для всех словоформ в парадигме слова. По крайней мере так в теории. На практике стемминг сводится к отбрасыванию частотных окончаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый известный стеммер - стеммер Портера (или snowball стеммер). \n",
    "Подробнее про стеммер Портера можно почитать вот тут - <https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340>  \n",
    "А совсем подробнее вот тут - <http://snowball.tartarus.org/algorithms/russian/stemmer.html>  \n",
    "Почему он так называется? Так назывался язык программирования, который Портер написал для стеммеров. Язык так называется в созвучие языку SNOBOL. Вот комментарий самого Портера:\n",
    "\n",
    "`Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed with the idea of calling it ‘strippergram’, but good sense has prevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the excellent string handling language of Messrs Farber, Griswold, Poage and Polonsky from the 1960s.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовые стеммеры для разных языков есть в nltk. Работают они вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Привет', 'привет'),\n",
       " ('!', '!'),\n",
       " ('Меня', 'мен'),\n",
       " ('зовут', 'зовут'),\n",
       " ('Денис', 'денис'),\n",
       " ('Кирьянов', 'кирьян'),\n",
       " (',', ','),\n",
       " ('я', 'я'),\n",
       " ('работаю', 'работа'),\n",
       " ('в', 'в'),\n",
       " ('Сбербанке', 'сбербанк'),\n",
       " ('и', 'и'),\n",
       " ('занимаюсь', 'занима'),\n",
       " ('проблемами', 'проблем'),\n",
       " ('обработки', 'обработк'),\n",
       " ('естественного', 'естествен'),\n",
       " ('языка', 'язык'),\n",
       " ('(', '('),\n",
       " ('NLP', 'NLP'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('Однажды', 'однажд'),\n",
       " ('нам', 'нам'),\n",
       " ('понадобилось', 'понадоб'),\n",
       " ('выбрать', 'выбра'),\n",
       " ('синтаксический', 'синтаксическ'),\n",
       " ('парсер', 'парсер'),\n",
       " ('для', 'для'),\n",
       " ('работы', 'работ'),\n",
       " ('с', 'с')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, stemmer.stem(word)) for word in word_tokenize(text)][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки стемминга достаточно очевидные:  \n",
    "1) с супплетивными формами или редкими окончаниями слова стемминг работать не умеет  \n",
    "2) к одной основе могут приводится разные слова  \n",
    "3) к разным основам могут сводиться формы одного слова  \n",
    "4) приставки не отбрасываются (иногда это нужно)\n",
    "5) Иногда получается бред типа \"меня\" -> \"мен\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поковыряйтесь со стеммером для английского."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_en = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_en.stem('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smarter'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_en.stem('smarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meet'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_en.stem('meeting') # а если это сущ. \"встреча\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кур'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"курить\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кур'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"куры\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация - это замена словоформы слова в парадигме на лемму. \n",
    "\n",
    "\n",
    "\n",
    "Например, для разных форм глагола леммой обычно является неопределенная форма, а для существительного форма мужского рода единственного числа. Это позволяет избавиться от недостатков стемминга (будет, был - одна лемма), (пролить, пролом - разные). Однако лемматизация значительно сложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К счастью есть готовые (хорошие?) лемматизаторы. Для русского основых варианта два: Mystem и Pymorphy. (Естественно, это уже старые технологии, они довольно плохо учитывают контекст, можете сами потестить разные штуки с омонимией).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Майстем работает немного лучше и сам токенизирует,\n",
    "поэтому можно в него засовывать сырой текст. НО! Работает медленно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['весь', 'этот', 'тип', 'становиться', 'быть', 'в', 'наш', 'цех']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mystem.lemmatize функция лемматизации в майстеме\n",
    "# сам объект mystem нужно заранее инициализировать\n",
    "# мы сделали это в начале тетрадки строчкой \"mystem = Mystem(entire_input=False)\"\n",
    "# entire_input=False удаляет пробелы и переносы строк из выдачи\n",
    "mystem.lemmatize('Все эти типы стали есть в нашем цехе')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки Mystem: это продукт Яндекса с некоторыми ограничениями на использование, больше он не развивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным достоинством Mystem является то, что он работает не с отдельными словами, а с целым предложением. При определении нужной леммы учитывается контекст, что позволяет во многих случаях разрешать омонимию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy - открытый и развивается (можно поучаствовать на гитхабе)\n",
    "\n",
    "Ссылка на репозиторий: https://github.com/kmike/pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него нет встроенной токенизации и он расценивает всё как слово. Когда есть несколько вариантов, он выдает их с вероятностостями, которые расчитатны на корпусе со снятой неоднозначностью (OpenCorpora, насколько я знаю). Это лучше стемминга, но хуже майстема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parse(word='все', tag=OpencorporaTag('ADJF,Apro plur,nomn'), normal_form='весь', score=0.357142, methods_stack=((<DictionaryAnalyzer>, 'все', 703, 20),)),\n",
       "  Parse(word='всё', tag=OpencorporaTag('ADJF,Apro neut,sing,nomn'), normal_form='весь', score=0.214285, methods_stack=((<DictionaryAnalyzer>, 'всё', 703, 14),)),\n",
       "  Parse(word='все', tag=OpencorporaTag('ADJF,Apro inan,plur,accs'), normal_form='весь', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'все', 703, 23),)),\n",
       "  Parse(word='всё', tag=OpencorporaTag('ADVB'), normal_form='всё', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'всё', 3, 0),)),\n",
       "  Parse(word='всё', tag=OpencorporaTag('ADJF,Apro neut,sing,accs'), normal_form='весь', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'всё', 703, 17),))],\n",
       " [Parse(word='эти', tag=OpencorporaTag('ADJF,Apro,Subx,Anph plur,nomn'), normal_form='этот', score=0.571428, methods_stack=((<DictionaryAnalyzer>, 'эти', 3099, 19),)),\n",
       "  Parse(word='эти', tag=OpencorporaTag('ADJF,Apro,Subx,Anph inan,plur,accs'), normal_form='этот', score=0.428571, methods_stack=((<DictionaryAnalyzer>, 'эти', 3099, 22),))],\n",
       " [Parse(word='типы', tag=OpencorporaTag('NOUN,inan,masc plur,accs'), normal_form='тип', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'типы', 33, 9),)),\n",
       "  Parse(word='типы', tag=OpencorporaTag('NOUN,inan,masc plur,nomn'), normal_form='тип', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'типы', 33, 6),)),\n",
       "  Parse(word='типы', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='тип', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'типы', 52, 6),))],\n",
       " [Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))],\n",
       " [Parse(word='есть', tag=OpencorporaTag('INTJ'), normal_form='есть', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'есть', 21, 0),)),\n",
       "  Parse(word='есть', tag=OpencorporaTag('VERB,impf,intr sing,3per,pres,indc'), normal_form='быть', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'есть', 587, 1),)),\n",
       "  Parse(word='есть', tag=OpencorporaTag('VERB,impf,intr plur,3per,pres,indc'), normal_form='быть', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'есть', 587, 3),)),\n",
       "  Parse(word='есть', tag=OpencorporaTag('INFN,impf,tran'), normal_form='есть', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'есть', 1383, 0),))],\n",
       " [Parse(word='в', tag=OpencorporaTag('PREP'), normal_form='в', score=0.999764, methods_stack=((<DictionaryAnalyzer>, 'в', 375, 0),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr sing,nomn'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 20),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr sing,gent'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 21),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr sing,datv'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 22),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr sing,accs'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 23),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr sing,ablt'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 24),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr sing,loct'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 25),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr plur,nomn'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 26),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr plur,gent'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 27),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr plur,datv'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 28),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr plur,accs'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 29),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr plur,ablt'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 30),)),\n",
       "  Parse(word='в', tag=OpencorporaTag('NOUN,inan,masc,Fixd,Abbr plur,loct'), normal_form='век', score=1.9e-05, methods_stack=((<DictionaryAnalyzer>, 'в', 666, 31),))],\n",
       " [Parse(word='нашем', tag=OpencorporaTag('ADJF,Apro masc,sing,loct'), normal_form='наш', score=0.703703, methods_stack=((<DictionaryAnalyzer>, 'нашем', 2047, 6),)),\n",
       "  Parse(word='нашем', tag=OpencorporaTag('ADJF,Apro neut,sing,loct'), normal_form='наш', score=0.296296, methods_stack=((<DictionaryAnalyzer>, 'нашем', 2047, 18),))],\n",
       " [Parse(word='цехе', tag=OpencorporaTag('NOUN,inan,masc sing,loct'), normal_form='цех', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'цехе', 3011, 5),))]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# основной метод - pymorphy.parse\n",
    "words_analized = [morph.parse(token) for token in word_tokenize('Все эти типы стали есть в нашем цехе')]\n",
    "words_analized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово -  все\n",
      "Разбор первого слова -  Parse(word='все', tag=OpencorporaTag('ADJF,Apro plur,nomn'), normal_form='весь', score=0.357142, methods_stack=((<DictionaryAnalyzer>, 'все', 703, 20),))\n",
      "Лемма первого слова -  весь\n",
      "Грамматическая информация первого слова -  ADJF,Apro plur,nomn\n",
      "Часть речи первого слова -  ADJF\n",
      "Род первого слова -  None\n",
      "Число первого слова -  plur\n",
      "Падеж первого слова -  nomn\n"
     ]
    }
   ],
   "source": [
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "print('Первое слово - ', words_analized[0][0].word)\n",
    "print('Разбор первого слова - ', words_analized[0][0])\n",
    "print('Лемма первого слова - ', words_analized[0][0].normal_form)\n",
    "print('Грамматическая информация первого слова - ', words_analized[0][0].tag)\n",
    "print('Часть речи первого слова - ', words_analized[0][0].tag.POS)\n",
    "print('Род первого слова - ', words_analized[0][0].tag.gender)\n",
    "print('Число первого слова - ', words_analized[0][0].tag.number)\n",
    "print('Падеж первого слова - ', words_analized[0][0].tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пунктуация часто совсем не нужна и поэтому можно выбросить её заранее. Если нужно обрабатывать много текста, это может немного ускорить процесс.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'это',\n",
       " 'случай',\n",
       " 'слово',\n",
       " 'вроде',\n",
       " 'и',\n",
       " 'не',\n",
       " 'пройти',\n",
       " 'фильтр',\n",
       " 'и',\n",
       " 'быть',\n",
       " 'удалить']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставим только буквенно-численные токены\n",
    "# Это не самый лучший вариант, так как удалятся сокращения с точкой, слова через дефис\n",
    "text = 'В этом случае слова вроде т.к. и по-другому не пройдут фильтр и будут удалены.'\n",
    "good_tokens = [word for word in word_tokenize(text) if word.isalnum()]\n",
    "[morph.parse(token)[0].normal_form for token in good_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно убрать стоп-слова (предлоги, союзы, местоимения, частотные слова). Сам термин стоп-слово происходит из информационного поиска.  \n",
    "Удаление таких слов позволяло сократить размер текста и не сильно испортить выдачу или даже улучшить её, поднимая релевантность документам со значимыми словами. Со временем от такой практики, в основном, отказались - память стала дешевой (и повились всякие алгоритмы для сокращения потребления памяти), а для учёта значимости придумали такую штуку, как IDF.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "with open('bmr_normed.json') as json_file:\n",
    "    bmr = json.load(json_file)\n",
    "    bmr_toked = []\n",
    "    for i, post in enumerate(bmr):\n",
    "        bmr_toked.extend([token.text for token in list(razdel_tokenize(post['text']))])\n",
    "        \n",
    "    cnt = Counter(bmr_toked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 29521)\n",
      "('.', 18754)\n",
      "('и', 8979)\n",
      "('в', 7111)\n",
      "('—', 6081)\n",
      "('не', 5202)\n",
      "('\"', 5083)\n",
      "(')', 4123)\n",
      "('...', 4032)\n",
      "('на', 3847)\n",
      "('!', 3351)\n",
      "('что', 2884)\n",
      "('с', 2853)\n",
      "(':', 2147)\n",
      "('(', 1837)\n",
      "('я', 1813)\n",
      "('?', 1710)\n",
      "('как', 1691)\n",
      "('И', 1665)\n",
      "('_', 1519)\n"
     ]
    }
   ],
   "source": [
    "print(*cnt.most_common(20), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрите, самыми частотными оказались знаки пунктуации, союзы и предлоги (из-за того, что мы не приводили к нижнему регистру, вылезло даже заглавное И)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во многих поисковых движках стоп-слова всё ещё используются. Часто их используют и в практических задачах (классификации, тематическом моделировании). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и, в, во, не, что, он, на, я, с, со, как, а, то, все, она, так, его, но, да, ты, к, у, же, вы, за, бы, по, только, ее, мне, было, вот, от, меня, еще, нет, о, из, ему, теперь, когда, даже, ну, вдруг, ли, если, уже, или, ни, быть, был, него, до, вас, нибудь, опять, уж, вам, ведь, там, потом, себя, ничего, ей, может, они, тут, где, есть, надо, ней, для, мы, тебя, их, чем, была, сам, чтоб, без, будто, чего, раз, тоже, себе, под, будет, ж, тогда, кто, этот, того, потому, этого, какой, совсем, ним, здесь, этом, один, почти, мой, тем, чтобы, нее, сейчас, были, куда, зачем, всех, никогда, можно, при, наконец, два, об, другой, хоть, после, над, больше, тот, через, эти, нас, про, всего, них, какая, много, разве, три, эту, моя, впрочем, хорошо, свою, этой, перед, иногда, лучше, чуть, том, нельзя, такой, им, более, всегда, конечно, всю, между\n"
     ]
    }
   ],
   "source": [
    "# стоп-слова есть в nltk\n",
    "stops = stopwords.words('russian')\n",
    "print(*stops, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список не идеальный и его можно расширять под свои задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['это', 'случай', 'слово', 'вроде', 'пройти', 'фильтр', 'удалить']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_normalized = [morph.parse(token)[0].normal_form for token in good_tokens]\n",
    "[word for word in words_normalized if word not in stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка для других языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорее всего вы будете работать в основном только с русским языком. Но знать, чем пользоваться для других на всякий случай - тоже полезно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk по умолчанию адаптирован под английский язык."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть еще одна библиотека, про которую стоит рассказать - [**SpaCy**](https://spacy.io/). Это многоцелевая многоязычная библиотека. Если вам понадобится серьезно работать с английским, то лучшим вариантом будет использовать SpaCy. Другие языки там тоже поддерживаются (см. документацию - ), но не настолько хорошо как английский язык. \n",
    "\n",
    "В SpaCy много всего и мы будем возвращаться к ней по ходу курса. Пока посмотрим на интрументы базовой предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (46.0.0.post20200309)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.43.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/Users/marimitchurina/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "^C\n",
      "Collecting de_core_news_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n",
      "\u001b[K     |███████████████████▍            | 9.1 MB 2.6 MB/s eta 0:00:03^C██████████████▊            | 9.2 MB 2.6 MB/s eta 0:00:03\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/req_command.py\", line 204, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 319, in run\n",
      "    reqs, check_supported_wheels=not options.target_dir\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 104, in resolve\n",
      "    req, requested_extras=()\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 434, in make_requirement_from_install_req\n",
      "    version=None,\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 205, in _make_candidate_from_link\n",
      "    version=version,\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 312, in __init__\n",
      "    version=version,\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 151, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 234, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 318, in _prepare_distribution\n",
      "    self._ireq, parallel_builds=True\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 552, in _prepare_linked_requirement\n",
      "    self.download_dir, hashes\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 243, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/network/download.py\", line 157, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/progress_bars.py\", line 156, in iter\n",
      "    self.next(len(x))  # noqa: B305\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/progress/__init__.py\", line 120, in next\n",
      "    self.update()\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/progress/bar.py\", line 83, in update\n",
      "    self.writeln(line)\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/progress/__init__.py\", line 103, in writeln\n",
      "    self.file.flush()\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/progress_bars.py\", line 106, in handle_sigint\n",
      "    self.finish()\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/progress_bars.py\", line 96, in finish\n",
      "    super().finish()  # type: ignore\n",
      "  File \"/Users/marimitchurina/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/progress/__init__.py\", line 107, in finish\n",
      "    print(file=self.file)\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для английского языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"Panda eats, shoots and leaves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda panda NOUN\n",
      "eats eat VERB\n",
      ", , PUNCT\n",
      "shoots shoot NOUN\n",
      "and and CCONJ\n",
      "leaves leave NOUN\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['the most salient features', 'our culture', 'so much bullshit', 'the opening words', 'the short book', 'Bullshit', 'the philosopher', 'Harry Frankfurt', 'the publication', 'this surprise bestseller', 'the rapid progress', 'research', 'artificial intelligence', 'us', 'our conception', 'bullshit', 'a hallmark', 'human speech', 'troubling implications', 'What', 'philosophical reflections', 'bullshit', 'algorithms', 'it', 'May', 'the company', 'Elon Musk', 'a new language model', 'Transformer', 'It', 'the tech world', 'storm', 'the surface', 'GPT-3', 'a supercharged version', 'the autocomplete feature', 'your smartphone', 'it', 'coherent text', 'an initial input', 'GPT-3’s text-generating abilities', 'anything', 'your phone', 'It', 'pronouns', 'translate', 'some forms', 'common-sense reasoning', 'It', 'fake news articles', 'humans', 'chance', 'a definition', 'it', 'a made-up word', 'a sentence', 'It', 'a paragraph', 'the style', 'a famous author', 'it', 'creative fiction', 'code', 'a program', 'a description', 'its function', 'It', 'queries', 'general knowledge', 'The list', 'WHO', 'hardly anyone', 'the hoax', 'this self-help blog post', 'GPT-3', 'the top', 'Hacker News', 'a popular news aggregation website', 'You', 'creative thinking', 'fun', 'GPT-3', 'you', 'much time', 'it', 'it', 'it', 'work', 'a marvel', 'engineering', 'its breathtaking scale', 'It', '175 billion parameters', 'the weights', 'the connections', 'the “neurons”or units', 'the network', '96 layers', 'It', 'embeddings', 'a vector space', '12,288 dimensions', 'it', 'hundreds', 'billions', 'words', 'a significant subset', 'the Internet', 'the entirety', 'English Wikipedia', 'countless books', 'a dizzying number', 'web pages', 'the final model', 'all accounts', 'GPT-3', 'a behemoth', 'the size', 'its network and training data', 'fundamental improvements', 'the years-old architecture', 'the model', 'unexpectedly remarkable performance', 'a range', 'complex tasks', 'the box', 'GPT-3', '“few-shot', 'some cases', 'a new task', 'any example', 'what', 'success']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для немецкого языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"Vor den Stadien habe ich bis jetzt zum Glück noch keine wüsten Szenen gesehen.\"\n",
    "        \"Vorstandschef Timotheus Höttges habe sich ausgesprochen optimistisch gezeigt, schrieb Analyst Robert Grindle in einer Studie vom Montag. \"\n",
    "        \"Während der dortigen Räterepublik war er nach dem Krieg in Künstlergruppen und Ausschüssen aktiv.\"\n",
    "        \"Welches Ergebnis die Diskussion auf EU-Ebene auch letztlich bringt, wichtig ist, dass die Preisentwicklung für die\"\n",
    "        \"Menschen verträglicher gestaltet wird“, so Gusenbauer.\"\n",
    "        \"Weitere Informationen unter www.schnippenburg.de sowie www.eisenzeithaus.de. Es gibt neue Nachrichten auf noz.de!\" \n",
    "        \"Jetzt die Startseite neu laden.\"\n",
    "        \"Der Initiative 'Zivilcourage', die sich jahrelang für das Denkmal in Form eines offenen \" \n",
    "        \"Der islamistischen Szene Thüringens wurden nach Angaben des Thüringer Innenministeriums \"\n",
    "        \"zuletzt etwa 125 Personen zugerechnet, der salafistischen Szene etwa 75 Personen.\"\n",
    "        \"Allerdings bestand er die EMV-Prüfung nicht, weil er Radios und DVB-T-Empfänger stört.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor Vor ADP\n",
      "den der DET\n",
      "Stadien Stadium NOUN\n",
      "habe habe AUX\n",
      "ich ich PRON\n",
      "bis bis ADP\n",
      "jetzt jetzt ADV\n",
      "zum zum ADP\n",
      "Glück Glück NOUN\n",
      "noch noch ADV\n",
      "keine kein DET\n",
      "wüsten wüst ADJ\n",
      "Szenen Szene NOUN\n",
      "gesehen sehen VERB\n",
      ". . PUNCT\n",
      "Vorstandschef Vorstandschef NOUN\n",
      "Timotheus Timotheus PROPN\n",
      "Höttges Höttges NOUN\n",
      "habe habe AUX\n",
      "sich sich PRON\n",
      "ausgesprochen aussprechen ADJ\n",
      "optimistisch optimistisch ADJ\n",
      "gezeigt zeigen VERB\n",
      ", , PUNCT\n",
      "schrieb schreiben VERB\n",
      "Analyst Analyst NOUN\n",
      "Robert Robert PROPN\n",
      "Grindle Grindle PROPN\n",
      "in in ADP\n",
      "einer einer DET\n",
      "Studie Studie NOUN\n",
      "vom vom ADP\n",
      "Montag Montag NOUN\n",
      ". . PUNCT\n",
      "Während während ADP\n",
      "der der DET\n",
      "dortigen dortig ADJ\n",
      "Räterepublik Räterepublik NOUN\n",
      "war sein AUX\n",
      "er ich PRON\n",
      "nach nach ADP\n",
      "dem der DET\n",
      "Krieg Krieg NOUN\n",
      "in in ADP\n",
      "Künstlergruppen Künstlergruppen NOUN\n",
      "und und CCONJ\n",
      "Ausschüssen Ausschuß NOUN\n",
      "aktiv aktiv ADJ\n",
      ". . PUNCT\n",
      "Welches welch DET\n",
      "Ergebnis Ergebnis NOUN\n",
      "die der DET\n",
      "Diskussion Diskussion NOUN\n",
      "auf auf ADP\n",
      "EU-Ebene EU-Ebene NOUN\n",
      "auch auch ADV\n",
      "letztlich letztlich ADV\n",
      "bringt bringen VERB\n",
      ", , PUNCT\n",
      "wichtig wichtig ADJ\n",
      "ist sein AUX\n",
      ", , PUNCT\n",
      "dass dass SCONJ\n",
      "die der DET\n",
      "Preisentwicklung Preisentwicklung NOUN\n",
      "für für ADP\n",
      "dieMenschen dieMenschen NOUN\n",
      "verträglicher verträglich ADJ\n",
      "gestaltet gestalten VERB\n",
      "wird werden AUX\n",
      "“ “ PUNCT\n",
      ", , PUNCT\n",
      "so so ADV\n",
      "Gusenbauer Gusenbauer NOUN\n",
      ". . PUNCT\n",
      "Weitere Weitere ADJ\n",
      "Informationen Information NOUN\n",
      "unter unter ADP\n",
      "www.schnippenburg.de www.schnippenburg.de PROPN\n",
      "sowie sowie CCONJ\n",
      "www.eisenzeithaus.de www.eisenzeithaus.de PROPN\n",
      ". . PUNCT\n",
      "Es ich PRON\n",
      "gibt geben VERB\n",
      "neue neue ADJ\n",
      "Nachrichten Nachricht NOUN\n",
      "auf auf ADP\n",
      "noz.de noz.de NOUN\n",
      "! ! PUNCT\n",
      "Jetzt Jetzt ADV\n",
      "die der DET\n",
      "Startseite Startseite NOUN\n",
      "neu neu ADJ\n",
      "laden laden VERB\n",
      ". . PUNCT\n",
      "Der der DET\n",
      "Initiative Initiative NOUN\n",
      "' ' PUNCT\n",
      "Zivilcourage Zivilcourage NOUN\n",
      "' ' PUNCT\n",
      ", , PUNCT\n",
      "die der PRON\n",
      "sich sich PRON\n",
      "jahrelang jahrelang ADJ\n",
      "für für ADP\n",
      "das der DET\n",
      "Denkmal Denkmal NOUN\n",
      "in in ADP\n",
      "Form Form NOUN\n",
      "eines ein DET\n",
      "offenen offen ADJ\n",
      "Der der DET\n",
      "islamistischen islamistischen ADJ\n",
      "Szene Szene NOUN\n",
      "Thüringens Thüringen PROPN\n",
      "wurden werden AUX\n",
      "nach nach ADP\n",
      "Angaben Angabe NOUN\n",
      "des der DET\n",
      "Thüringer Thüringer ADJ\n",
      "Innenministeriums Innenministerium NOUN\n",
      "zuletzt zuletzt ADV\n",
      "etwa etwa ADV\n",
      "125 125 NUM\n",
      "Personen Person NOUN\n",
      "zugerechnet zurechnen VERB\n",
      ", , PUNCT\n",
      "der der DET\n",
      "salafistischen salafistischen ADJ\n",
      "Szene Szene NOUN\n",
      "etwa etwa ADV\n",
      "75 75 NUM\n",
      "Personen Person NOUN\n",
      ". . PUNCT\n",
      "Allerdings Allerdings ADV\n",
      "bestand bestand VERB\n",
      "er ich PRON\n",
      "die der DET\n",
      "EMV-Prüfung EMV-Prüfung NOUN\n",
      "nicht nicht PART\n",
      ", , PUNCT\n",
      "weil weil SCONJ\n",
      "er ich PRON\n",
      "Radios Radio NOUN\n",
      "und und CCONJ\n",
      "DVB-T-Empfänger DVB-T-Empfänger PROPN\n",
      "stört stören VERB\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['den Stadien', 'ich', 'Glück', 'noch keine wüsten Szenen', 'Vorstandschef Timotheus Höttges', 'sich', 'Analyst Robert Grindle', 'einer Studie', 'Montag', 'der dortigen Räterepublik', 'er', 'dem Krieg', 'Künstlergruppen', 'Ausschüssen', 'Welches Ergebnis', 'die Diskussion', 'EU-Ebene', 'die Preisentwicklung', 'dieMenschen', 'Welches Ergebnis die Diskussion auf EU-Ebene auch letztlich bringt, wichtig ist, dass die Preisentwicklung für dieMenschen verträglicher gestaltet wird“, so Gusenbauer', 'Weitere Informationen', 'www.schnippenburg.de', 'www.eisenzeithaus.de', 'neue Nachrichten', 'noz.de', 'die Startseite', \"Der Initiative 'Zivilcourage\", 'die', 'sich', 'das Denkmal', 'Form', 'eines offenen Der islamistischen Szene', 'Thüringens', 'Angaben', 'des Thüringer Innenministeriums', 'etwa 125 Personen', 'der salafistischen Szene', 'er', 'die EMV-Prüfung', 'er', 'Radios', 'DVB-T-Empfänger']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть русскоязычный форк spacy - https://github.com/buriy/spacy-ru Но это конечно не полноценный spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
