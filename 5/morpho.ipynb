{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/nlpub/pymystem3\n",
    "# кто в колабе - pip install pymystem3. Не будет фичи get_pos\n",
    "# pip install rnnmorph==0.4.0\n",
    "# pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "mystem = Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem.lemmatize('Бои у Сопоцкина и Друскеник закончились отступлением германцев.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По дефолту разрешает омонимию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem.lemmatize('сорок девять')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = mystem.analyze('сорок девять')\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[0]['analysis'][0]['lex'] # номер слова, падаем в анализ, gr - грамматика, lex - лемма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 методы у mystem:\n",
    "- lemmatize - возвращает список лемм (просто список строк)\n",
    "- analyze - возвращает сложнуюю структуру с POS-тегами (список словарей списков словарей)\n",
    "- get_pos - возвращает часть речи для конкретного слова из уже проанализированного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Взять любой абзац из lenta_tiny, привести все слова к леммам, вывести уникальные слова списком по алфавиту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Задача\n",
    "Взять lenta_tiny и вывести всех бастардов (т.е. OOV-слова) (не забудьте заменить '\\n' на пробелы!). Сколько их получилось? Сколько уникальных? Что это за слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Посчитайте кол-во всех сущ., глаголов, прилагательных и наречий. Исп. метод get_pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Задача\n",
    "Посчитайте, слова какой чаcти речи чаще всего идут перед существительными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNMorph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передаем уже токенизированный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "forms = predictor.predict([\"я\", 'плачу', 'за', 'тебя'])\n",
    "\n",
    "print(*forms, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вывести лемму слова по его индексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms[1].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или часть речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms[1].pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Посчитайте кол-во всех сущ., глаголов, прилагательных и наречий в lenta_tiny. Сравните c выводом майстема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONLLU файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тагсет для русского языка: https://github.com/dialogue-evaluation/GramEval2020/blob/master/UDtagset/UD-Russian_tagset.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У токенов есть атрибуты - те самые колонки, на которые разбиты файлы conllu\n",
    "- token.id - номер\n",
    "- token.form - словоформа\n",
    "- token.lemma\n",
    "- token.upos - универсальный pos, или часть речи\n",
    "- token.feats - грам значения и граммемы\n",
    "- token.head - расстояние от корня предложения, с помощью этого графические редакторы для UD и строят деревья\n",
    "- token.deprel - названия синтакс отношений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "train = pyconll.load_from_file('social.conllu') ## путь до файла social.conllu\n",
    "\n",
    "particle = []\n",
    "\n",
    "\n",
    "for sentence in train:  # итерируемся по предложениям\n",
    "    for token in sentence:  # переходим на уровень токенов\n",
    "\n",
    "        if token.upos == 'PART':   # допустим, мы хотим найти все частицы в нашем файле\n",
    "            particle.append(token.form)\n",
    "\n",
    "            # if token.feats['Number'] == {'Plur'}: - если хотите посмотреть конкретную граммему\n",
    "\n",
    "print(particle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Выведите все союзы из файла social в нижнем регистре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Задача\n",
    "Выведите все причастия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
