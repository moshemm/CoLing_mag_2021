{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/nlpub/pymystem3\n",
    "# Если не скачивается с гитхаба - pip install pymystem3. Не будет фичи get_pos\n",
    "# pip install rnnmorph\n",
    "# pip install pyconll\n",
    "\n",
    "# Чтобы снести - pip uninstall ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "mystem = Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONLLU файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тагсет для русского языка: https://github.com/dialogue-evaluation/GramEval2020/blob/master/UDtagset/UD-Russian_tagset.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У токенов есть атрибуты - те самые колонки, на которые разбиты файлы conllu\n",
    "- token.id - номер\n",
    "- token.form - словоформа\n",
    "- token.lemma\n",
    "- token.upos - универсальный pos, или часть речи\n",
    "- token.feats - грам значения и граммемы\n",
    "- token.head - расстояние от корня предложения, с помощью этого графические редакторы для UD и строят деревья\n",
    "- token.deprel - названия синтакс отношений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'же', 'и', 'то', 'только', 'да', 'Просто', 'ну', 'даже', 'ведь', 'это', 'Не', 'бы', 'таки', 'как', 'просто', 'ладно', 'Нет', 'вот', 'Ведь', 'Хотя', 'Ну', 'Даже', 'Только', 'ни', 'уж', 'всеж', 'не', 'ли', 'типа', 'Да', 'Вот', 'разве'}\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "\n",
    "train = pyconll.load_from_file('social.conllu') ## путь до файла social.conllu\n",
    "\n",
    "particle = []\n",
    "\n",
    "\n",
    "for sentence in train:  # итерируемся по предложениям\n",
    "    for token in sentence:  # переходим на уровень токенов\n",
    "\n",
    "        if token.upos == 'PART':   # допустим, мы хотим найти все частицы в нашем файле\n",
    "            particle.append(token.form)\n",
    "\n",
    "            # if token.feats['Number'] == {'Plur'}: - если хотите посмотреть конкретную граммему\n",
    "\n",
    "print(set(particle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Выведите все союзы из файла social в нижнем регистре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 44),\n",
       " ('а', 14),\n",
       " ('как', 11),\n",
       " ('что', 9),\n",
       " ('если', 8),\n",
       " ('но', 8),\n",
       " ('когда', 5),\n",
       " ('то', 3),\n",
       " ('или', 2),\n",
       " ('либо', 2),\n",
       " ('да', 2),\n",
       " ('пока', 1),\n",
       " ('так', 1),\n",
       " ('чтоб', 1),\n",
       " ('плюс', 1),\n",
       " ('чем', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pyconll.load_from_file('social.conllu') ## путь до файла social.conllu\n",
    "\n",
    "conj = []\n",
    "\n",
    "for sentence in train:  # итерируемся по предложениям\n",
    "    for token in sentence:  # переходим на уровень токенов\n",
    "\n",
    "        if token.upos.endswith('CONJ'):   # допустим, мы хотим найти все частицы в нашем файле\n",
    "            conj.append(token.form.lower())\n",
    "\n",
    "Counter(conj).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Задача\n",
    "Выведите все причастия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обитый',\n",
       " 'заложена',\n",
       " 'порождённая',\n",
       " 'искаженными',\n",
       " 'опустевшим',\n",
       " 'привлекающий',\n",
       " 'выражающий',\n",
       " 'созданную',\n",
       " 'удивлен']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pyconll.load_from_file('social.conllu') ## путь до файла social.conllu\n",
    "\n",
    "part = []\n",
    "\n",
    "for sentence in train:  # итерируемся по предложениям\n",
    "    for token in sentence:  # переходим на уровень токенов\n",
    "\n",
    "        if token.upos == 'VERB' and 'VerbForm' in token.feats:\n",
    "            if token.feats['VerbForm'] == {'Part'}:\n",
    "                part.append(token.form.lower())\n",
    "\n",
    "part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystem сам токенизирует текст (моя ж ты прелесть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['бой',\n",
       " 'у',\n",
       " 'сопоцкин',\n",
       " 'и',\n",
       " 'друскеник',\n",
       " 'заканчиваться',\n",
       " 'отступление',\n",
       " 'германец']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.lemmatize('Бои у Сопоцкина и Друскеник закончились отступлением германцев.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По дефолту разрешает омонимию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сорок', 'девять']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.lemmatize('сорок девять')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'сорок', 'gr': 'NUM=(вин|им)'}], 'text': 'сорок'},\n",
       " {'analysis': [{'lex': 'девять', 'gr': 'NUM=(вин|им)'}], 'text': 'девять'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = mystem.analyze('сорок девять')\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сорок'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[0]['analysis'][0]['lex'] # номер слова, падаем в анализ, gr - грамматика, lex - лемма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUM'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 метода у mystem:\n",
    "- lemmatize - возвращает список лемм (просто список строк)\n",
    "- analyze - возвращает сложнуюю структуру с POS-тегами (список словарей списков словарей)\n",
    "- get_pos - возвращает часть речи для конкретного слова из уже проанализированного. (Если вы скачали mystem с гитхаба)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Взять любой абзац из lenta_tiny, привести все слова к леммам, вывести уникальные слова списком по алфавиту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Задача\n",
    "Взять lenta_tiny и вывести всех бастардов (т.е. OOV-слова) (не забудьте заменить '\\n' на пробелы!). Сколько их получилось? Сколько уникальных? Что это за слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Посчитайте кол-во всех сущ., глаголов, прилагательных и наречий. Исп. метод get_pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Задача\n",
    "Посчитайте, слова какой чаcти речи чаще всего идут перед существительными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNMorph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передаем уже токенизированный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<normal_form=я; word=я; pos=PRON; tag=Case=Nom|Number=Sing|Person=1; score=1.0000>\n",
      "<normal_form=плакать; word=плачу; pos=VERB; tag=Mood=Ind|Number=Sing|Person=1|Tense=Notpast|VerbForm=Fin|Voice=Act; score=0.9998>\n",
      "<normal_form=за; word=за; pos=ADP; tag=_; score=1.0000>\n",
      "<normal_form=ты; word=тебя; pos=PRON; tag=Case=Acc|Number=Sing|Person=2; score=0.9962>\n"
     ]
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "forms = predictor.predict([\"я\", 'плачу', 'за', 'тебя'])\n",
    "\n",
    "print(*forms, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вывести лемму слова по его индексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'плакать'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forms[1].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или часть речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forms[1].pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача\n",
    "Посчитайте кол-во всех сущ., глаголов, прилагательных и наречий в lenta_tiny. Сравните c выводом майстема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
